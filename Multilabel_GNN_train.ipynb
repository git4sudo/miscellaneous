{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilabel_GNN_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eNCR6VcqUlc_"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phPvDF_WTeH9"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP36Mh8FMFfb",
        "outputId": "6d6fd88e-2938-4234-d09a-72ba718cbbcb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHWafOusMhd5",
        "outputId": "136f63de-59ca-44f6-92b6-2827d279c34d"
      },
      "source": [
        "cd /content/drive/MyDrive/NLP/Graph Multilabel Classification/MAGNET-main"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP/Graph Multilabel Classification/MAGNET-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nuv_CdufMoD6",
        "outputId": "7a72ab93-8014-4a7c-a550-cdb0086b18b8"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/NLP/Graph Multilabel Classification/MAGNET-main'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeHfYDNGoGka",
        "outputId": "3813af9b-a8a8-4ec2-e91c-0efca2183041"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import reuters\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from models import MAGNET"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2EG3jePzrE9"
      },
      "source": [
        "with ZipFile('./glove.6B.zip', 'r') as file:\n",
        "   file.extractall(path='./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xLEwWKB6QtL"
      },
      "source": [
        "#Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8H5bRTsHPL"
      },
      "source": [
        "#Build adjacency matrix based on Co-Occurencies label\n",
        "def buildAdjacencyCOOC(data_label):\n",
        "  adj = data_label.T.dot(data_label).astype('float')\n",
        "  for i in range(len(adj)):\n",
        "    adj[i] = adj[i] / adj[i,i]\n",
        "  \n",
        "  return torch.from_numpy(adj.astype('float32'))\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Text cleaning function\n",
        "def preprocessingText(text, stop=stop):\n",
        "  text = text.lower() #text to lowercase\n",
        "  text = re.sub(r'&lt;', '', text) #remove '&lt;' tag\n",
        "  text = re.sub(r'<.*?>', '', text) #remove html\n",
        "  text = re.sub(r'[0-9]+', '', text) #remove number\n",
        "  text = \" \".join([word for word in text.split() if word not in stop]) #remove stopwords\n",
        "  text = re.sub(r'[^\\w\\s]', '', text) #remove punctiation\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', '', text) #remove non ASCII strings\n",
        "  for c in ['\\r', '\\n', '\\t'] :\n",
        "    text = re.sub(c, ' ', text) #replace newline and tab with tabs\n",
        "  text = re.sub('\\s+', ' ', text) #replace multiple spaces with one space\n",
        "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "#Load Word Representation Vector\n",
        "def loadWRVModel(File):\n",
        "    print(\"Loading Word Representation Vector Model\")\n",
        "    f = open(File,'r')\n",
        "    WRVModel = {}\n",
        "    for line in f:\n",
        "        splitLines = line.split()\n",
        "        word = splitLines[0]\n",
        "        try:\n",
        "          wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
        "        except:\n",
        "          print(splitLines[1:])\n",
        "          print(len(splitLines[1:]))\n",
        "          break\n",
        "        WRVModel[word] = wordEmbedding\n",
        "    print(len(WRVModel),\" words loaded!\")\n",
        "    return WRVModel\n",
        "\n",
        "\n",
        "def check_accuracy(model, label_embedding, X, y):\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(X, label_embedding)\n",
        "    y_pred = torch.sigmoid(out.detach()).round().cpu()\n",
        "    f1score = f1_score(y, y_pred, average='micro')\n",
        "    hammingloss = hamming_loss(y, y_pred)\n",
        "  \n",
        "  return hammingloss, f1score\n",
        "\n",
        "def train(model,\n",
        "          X_train,\n",
        "          X_test,\n",
        "          label_embedding,\n",
        "          y_train,\n",
        "          y_test,\n",
        "          total_epoch=250,\n",
        "          batch_size=250,\n",
        "          learning_rate=0.001,\n",
        "          save_path='./model.pt',\n",
        "          state=None):\n",
        "  \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)\n",
        "  X_test = X_test.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  label_embedding= label_embedding.to(device)\n",
        "\n",
        "  if state:\n",
        "    state = torch.load(state)\n",
        "    model = model.load_state_dict(state['last_model'])\n",
        "    optimizer = optimizer.load_state_dict(state['optimizer'])\n",
        "  \n",
        "  else:\n",
        "    model = model.to(device)\n",
        "    state = dict()\n",
        "    state['microf1'] = []\n",
        "    state['hammingloss'] = []\n",
        "    state['val_hammingloss'] = []\n",
        "    state['val_microf1'] = []\n",
        "    state['epoch_time'] = []\n",
        "    \n",
        "  epoch = 1\n",
        "  \n",
        "  best_train = 0\n",
        "  best_val = 0\n",
        "  \n",
        "  while epoch <= total_epoch:\n",
        "    running_loss = 0\n",
        "    y_pred = []\n",
        "    epoch_time = 0\n",
        "    model.train()\n",
        "    for index, (X, y) in enumerate(train_data):\n",
        "      \n",
        "      t = time.time()\n",
        "\n",
        "      #forward\n",
        "      out = model(X.to(device), label_embedding)\n",
        "      loss = criterion(out, y.to(device))\n",
        "\n",
        "      #backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "      #update\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_time += time.time() - t\n",
        "      y_pred.append(torch.sigmoid(out.detach()).round().cpu())\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    y_pred = torch.vstack(y_pred)\n",
        "    f1score = f1_score(y_train, y_pred, average='micro')\n",
        "    hammingloss = hamming_loss(y_train, y_pred)\n",
        "    val_hamming, val_f1score = check_accuracy(model, label_embedding, X_test, y_test)\n",
        "\n",
        "    state['microf1'].append(f1score)\n",
        "    state['hammingloss'].append(hammingloss)\n",
        "    state['val_microf1'].append(val_f1score)\n",
        "    state['epoch_time'].append(epoch_time)\n",
        "    state['val_hammingloss'].append(val_hamming)\n",
        "\n",
        "    state['optimizer'] = optimizer.state_dict()\n",
        "    state['last_model'] = model.state_dict()\n",
        "    \n",
        "    \n",
        "    if(best_train < f1score):\n",
        "      state['model_best_train'] = copy.deepcopy(model.state_dict())\n",
        "      best_train = f1score\n",
        "      state['best_train'] = best_train\n",
        "    \n",
        "    if(best_val < val_f1score):\n",
        "      state['model_best_val'] = copy.deepcopy(model.state_dict())\n",
        "      best_val = val_f1score\n",
        "      state['best_val'] = best_val\n",
        "\n",
        "    torch.save(state, save_path)\n",
        "    print('epoch:{} loss:{:.5f} hamming_loss:{:.5f} micro_f1score:{:.5f} val_hamming_loss:{:.5f} val_micro_f1score:{:.5f}'.\n",
        "          format(epoch, running_loss, hammingloss, f1score, val_hamming, val_f1score))\n",
        "    epoch+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgZvCUaKY-xZ"
      },
      "source": [
        "#Load Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1PuvFDqT2Dv"
      },
      "source": [
        "## Load Raw Dataset (Reuters-21578)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD3nYHOGiwSQ",
        "outputId": "7e7bc7d8-c596-4a77-b1be-660003643493"
      },
      "source": [
        "data_train = pd.read_pickle('./train.pickle')\n",
        "data_test = pd.read_pickle('./test.pickle')\n",
        "\n",
        "text_train = data_train.text.values\n",
        "text_test = data_test.text.values\n",
        "\n",
        "y_train = torch.from_numpy(np.vstack(data_train.onehot_label.values)).float()\n",
        "y_test = torch.from_numpy(np.vstack(data_test.onehot_label.values)).float()\n",
        "\n",
        "print('Train label shape {}'.format(y_train.shape))\n",
        "print('Test label shape {}'.format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train label shape torch.Size([7769, 90])\n",
            "Test label shape torch.Size([3019, 90])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O07hOLy6TKKL",
        "outputId": "c025defc-d84f-45a4-c810-dbbd4316f5a0"
      },
      "source": [
        "len(data_train['onehot_label'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "kYUAK4c8TkpN",
        "outputId": "30282163-6abf-42b7-b332-5b6023c58ac9"
      },
      "source": [
        "data_train['text'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'BAHIA COCOA REVIEW\\n  Showers continued throughout the week in\\n  the Bahia cocoa zone, alleviating the drought since early\\n  January and improving prospects for the coming temporao,\\n  although normal humidity levels have not been restored,\\n  Comissaria Smith said in its weekly review.\\n      The dry period means the temporao will be late this year.\\n      Arrivals for the week ended February 22 were 155,221 bags\\n  of 60 kilos making a cumulative total for the season of 5.93\\n  mln against 5.81 at the same stage last year. Again it seems\\n  that cocoa delivered earlier on consignment was included in the\\n  arrivals figures.\\n      Comissaria Smith said there is still some doubt as to how\\n  much old crop cocoa is still available as harvesting has\\n  practically come to an end. With total Bahia crop estimates\\n  around 6.4 mln bags and sales standing at almost 6.2 mln there\\n  are a few hundred thousand bags still in the hands of farmers,\\n  middlemen, exporters and processors.\\n      There are doubts as to how much of this cocoa would be fit\\n  for export as shippers are now experiencing dificulties in\\n  obtaining +Bahia superior+ certificates.\\n      In view of the lower quality over recent weeks farmers have\\n  sold a good part of their cocoa held on consignment.\\n      Comissaria Smith said spot bean prices rose to 340 to 350\\n  cruzados per arroba of 15 kilos.\\n      Bean shippers were reluctant to offer nearby shipment and\\n  only limited sales were booked for March shipment at 1,750 to\\n  1,780 dlrs per tonne to ports to be named.\\n      New crop sales were also light and all to open ports with\\n  June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\n  under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\n  per tonne FOB.\\n      Routine sales of butter were made. March/April sold at\\n  4,340, 4,345 and 4,350 dlrs.\\n      April/May butter went at 2.27 times New York May, June/July\\n  at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n  2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n  2.27 times New York Dec, Comissaria Smith said.\\n      Destinations were the U.S., Covertible currency areas,\\n  Uruguay and open ports.\\n      Cake sales were registered at 785 to 995 dlrs for\\n  March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\n  New York Dec for Oct/Dec.\\n      Buyers were the U.S., Argentina, Uruguay and convertible\\n  currency areas.\\n      Liquor sales were limited with March/April selling at 2,325\\n  and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\n  York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\n  Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\n  said.\\n      Total Bahia sales are currently estimated at 6.13 mln bags\\n  against the 1986/87 crop and 1.06 mln bags against the 1987/88\\n  crop.\\n      Final figures for the period to February 28 are expected to\\n  be published by the Brazilian Cocoa Trade Commission after\\n  carnival which ends midday on February 27.\\n  \\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58ztm_oqT7_8",
        "outputId": "9f71bd20-1323-42f4-e4ed-3e708ba104cb"
      },
      "source": [
        "data_train['label']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    [cocoa]\n",
              "1                      [acq]\n",
              "2             [money-supply]\n",
              "3                      [acq]\n",
              "4                     [earn]\n",
              "                ...         \n",
              "7764    [interest, money-fx]\n",
              "7765                  [earn]\n",
              "7766                  [earn]\n",
              "7767                  [earn]\n",
              "7768                  [earn]\n",
              "Name: label, Length: 7769, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ-0B4vnZJpB"
      },
      "source": [
        "#Load MultilabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJ0YUzGZTO-"
      },
      "source": [
        "with open('./multilabelbinarizer.pickle', 'rb') as file:\n",
        "  mlb = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RminIPRXkY8"
      },
      "source": [
        "#Build Word Representation Vector Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3L7jlurXpjk",
        "outputId": "8d5564e4-46dd-4927-c3dd-cdf68651806a"
      },
      "source": [
        "WRVModel = loadWRVModel('./glove.6B.300d.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Word Representation Vector Model\n",
            "400001  words loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjxasUuZ4lRi"
      },
      "source": [
        "#Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq5ref4z4oCq"
      },
      "source": [
        "## Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIAPLVCmox6A",
        "outputId": "c6ac59af-4138-40a2-b5cc-a292ad53b07e"
      },
      "source": [
        "preprocessed_text_train = [preprocessingText(text) for text in text_train]\n",
        "preprocessed_text_test = [preprocessingText(text) for text in text_test]\n",
        "\n",
        "print('BEFORE CLEANING: {}'.format(text_train[0]))\n",
        "print('AFTER CLEANING: {}'.format(preprocessed_text_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE CLEANING: BAHIA COCOA REVIEW\n",
            "  Showers continued throughout the week in\n",
            "  the Bahia cocoa zone, alleviating the drought since early\n",
            "  January and improving prospects for the coming temporao,\n",
            "  although normal humidity levels have not been restored,\n",
            "  Comissaria Smith said in its weekly review.\n",
            "      The dry period means the temporao will be late this year.\n",
            "      Arrivals for the week ended February 22 were 155,221 bags\n",
            "  of 60 kilos making a cumulative total for the season of 5.93\n",
            "  mln against 5.81 at the same stage last year. Again it seems\n",
            "  that cocoa delivered earlier on consignment was included in the\n",
            "  arrivals figures.\n",
            "      Comissaria Smith said there is still some doubt as to how\n",
            "  much old crop cocoa is still available as harvesting has\n",
            "  practically come to an end. With total Bahia crop estimates\n",
            "  around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
            "  are a few hundred thousand bags still in the hands of farmers,\n",
            "  middlemen, exporters and processors.\n",
            "      There are doubts as to how much of this cocoa would be fit\n",
            "  for export as shippers are now experiencing dificulties in\n",
            "  obtaining +Bahia superior+ certificates.\n",
            "      In view of the lower quality over recent weeks farmers have\n",
            "  sold a good part of their cocoa held on consignment.\n",
            "      Comissaria Smith said spot bean prices rose to 340 to 350\n",
            "  cruzados per arroba of 15 kilos.\n",
            "      Bean shippers were reluctant to offer nearby shipment and\n",
            "  only limited sales were booked for March shipment at 1,750 to\n",
            "  1,780 dlrs per tonne to ports to be named.\n",
            "      New crop sales were also light and all to open ports with\n",
            "  June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\n",
            "  under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\n",
            "  per tonne FOB.\n",
            "      Routine sales of butter were made. March/April sold at\n",
            "  4,340, 4,345 and 4,350 dlrs.\n",
            "      April/May butter went at 2.27 times New York May, June/July\n",
            "  at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\n",
            "  2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\n",
            "  2.27 times New York Dec, Comissaria Smith said.\n",
            "      Destinations were the U.S., Covertible currency areas,\n",
            "  Uruguay and open ports.\n",
            "      Cake sales were registered at 785 to 995 dlrs for\n",
            "  March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\n",
            "  New York Dec for Oct/Dec.\n",
            "      Buyers were the U.S., Argentina, Uruguay and convertible\n",
            "  currency areas.\n",
            "      Liquor sales were limited with March/April selling at 2,325\n",
            "  and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\n",
            "  York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\n",
            "  Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\n",
            "  said.\n",
            "      Total Bahia sales are currently estimated at 6.13 mln bags\n",
            "  against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
            "  crop.\n",
            "      Final figures for the period to February 28 are expected to\n",
            "  be published by the Brazilian Cocoa Trade Commission after\n",
            "  carnival which ends midday on February 27.\n",
            "  \n",
            "\n",
            "\n",
            "AFTER CLEANING: bahia cocoa review shower continued throughout week bahia cocoa zone alleviating drought since early january improving prospect coming temporao although normal humidity level restored comissaria smith said weekly review dry period mean temporao late year arrival week ended february bag kilo making cumulative total season mln stage last year seems cocoa delivered earlier consignment included arrival figure comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimate around mln bag sale standing almost mln hundred thousand bag still hand farmer middleman exporter processor doubt much cocoa would fit export shipper experiencing dificulties obtaining bahia superior certificate view lower quality recent week farmer sold good part cocoa held consignment comissaria smith said spot bean price rose cruzados per arroba kilo bean shipper reluctant offer nearby shipment limited sale booked march shipment dlrs per tonne port named new crop sale also light open port junejuly going dlrs dlrs new york july augsept dlrs per tonne fob routine sale butter made marchapril sold dlrs aprilmay butter went time new york may junejuly dlrs augsept dlrs time new york sept octdec dlrs time new york dec comissaria smith said destination u covertible currency area uruguay open port cake sale registered dlrs marchapril dlrs may dlrs aug time new york dec octdec buyer u argentina uruguay convertible currency area liquor sale limited marchapril selling dlrs junejuly dlrs time new york july augsept dlrs time new york sept octdec time new york dec comissaria smith said total bahia sale currently estimated mln bag crop mln bag crop final figure period february expected published brazilian cocoa trade commission carnival end midday february\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6CbF8ci4yW2"
      },
      "source": [
        "##Text to Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8rCpqQC4OHq"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_text_train)\n",
        "\n",
        "sequences_text_train = tokenizer.texts_to_sequences(preprocessed_text_train)\n",
        "sequences_text_test = tokenizer.texts_to_sequences(preprocessed_text_test)\n",
        "\n",
        "X_train = torch.from_numpy(pad_sequences(sequences_text_train, maxlen=128))\n",
        "X_test = torch.from_numpy(pad_sequences(sequences_text_test, maxlen=128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC7bTKTaU7UC",
        "outputId": "7451f88f-b1ad-4cf5-dd59-30a62cf007a3"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7769, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZi_UZbg43uw"
      },
      "source": [
        "## Build Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7_sCNk3vQvD",
        "outputId": "e8a6b7a9-dcb0-4798-a55b-3540d2249145"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = torch.zeros(VOCAB_SIZE, 300)\n",
        "\n",
        "unk = 0\n",
        "for i in range(1, VOCAB_SIZE):\n",
        "  word = tokenizer.index_word[i]\n",
        "  if word in WRVModel.keys():\n",
        "    embedding_matrix[i] = torch.from_numpy(WRVModel[word]).float()\n",
        "  else:\n",
        "    unk +=1\n",
        "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
        "print('TOTAL OF UNKNOWN WORD : {}'.format(unk))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB_SIZE : 25306\n",
            "TOTAL OF UNKNOWN WORD : 6568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZHdtuWH6kiT"
      },
      "source": [
        "#Preparing Graph Attention Networks Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxTcQ_u6sUc"
      },
      "source": [
        "## Label Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URGNwS1hP_fr",
        "outputId": "698df7b0-23b9-4131-a286-d2f637276e0d"
      },
      "source": [
        "label_embedding = torch.zeros(90,300)\n",
        "\n",
        "for index, label in enumerate(mlb.classes_):\n",
        "  words = label.split('-')\n",
        "  num_of_words = len(words)\n",
        "\n",
        "  for sublabel in words:\n",
        "    if sublabel in WRVModel.keys():\n",
        "      label_embedding[index] +=  torch.from_numpy(WRVModel[sublabel])\n",
        "  label_embedding[index] = label_embedding[index]/num_of_words\n",
        "\n",
        "print(label_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1796, -0.1051, -0.5564,  ..., -0.0633,  0.3732, -0.2873],\n",
            "        [ 0.1101,  0.4061,  0.2036,  ..., -0.1957, -0.4627,  0.6931],\n",
            "        [-0.3568, -0.1348,  0.0790,  ..., -0.0384,  0.2948,  0.1996],\n",
            "        ...,\n",
            "        [-0.1446,  0.0594, -0.1450,  ..., -0.0334,  0.1966,  0.4136],\n",
            "        [-0.5990, -0.3234, -0.2749,  ...,  0.6343,  0.5300,  0.0299],\n",
            "        [-0.4541, -0.1300, -0.5178,  ..., -1.1637, -0.2056, -0.3177]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvT83CK66F-"
      },
      "source": [
        "##Adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhYLvfvTtZOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5937d160-b41c-4202-ff2d-a26378c3d39d"
      },
      "source": [
        "adjacency = buildAdjacencyCOOC(y_train.numpy())\n",
        "print(adjacency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],\n",
            "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0286],\n",
            "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
            "        [0.0952, 0.0476, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0dSc09x7lGm"
      },
      "source": [
        "#Preparing DataLoader and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uoQlm38OS5S"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vePGeWyvocZ"
      },
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x  = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi7g_VSYOYny"
      },
      "source": [
        "##Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apRrzy6cWzN5"
      },
      "source": [
        "model = MAGNET(300, 250, adjacency, embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvY8vULwSbXB"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTh-66uJSnij"
      },
      "source": [
        "##Configure Save PATH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC2_KVL5SfG-"
      },
      "source": [
        "save_path = './train_result.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwv_OW8zSx7W"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XeYrWVgvYrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d662f9-6e29-417b-e6bd-925c27f0f4d9"
      },
      "source": [
        "train(model, X_train, X_test, label_embedding, y_train, y_test, save_path=save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1 loss:4.14235 hamming_loss:0.02988 micro_f1score:0.01444 val_hamming_loss:0.01378 val_micro_f1score:0.00000\n",
            "epoch:2 loss:2.04930 hamming_loss:0.01370 micro_f1score:0.00083 val_hamming_loss:0.01378 val_micro_f1score:0.00000\n",
            "epoch:3 loss:1.67586 hamming_loss:0.01290 micro_f1score:0.17237 val_hamming_loss:0.01148 val_micro_f1score:0.37600\n",
            "epoch:4 loss:1.52677 hamming_loss:0.01261 micro_f1score:0.31292 val_hamming_loss:0.01233 val_micro_f1score:0.38727\n",
            "epoch:5 loss:1.43648 hamming_loss:0.01189 micro_f1score:0.36497 val_hamming_loss:0.01039 val_micro_f1score:0.41734\n",
            "epoch:6 loss:1.29117 hamming_loss:0.01040 micro_f1score:0.44068 val_hamming_loss:0.01007 val_micro_f1score:0.42929\n",
            "epoch:7 loss:1.18438 hamming_loss:0.00903 micro_f1score:0.54492 val_hamming_loss:0.00801 val_micro_f1score:0.61462\n",
            "epoch:8 loss:1.14319 hamming_loss:0.00972 micro_f1score:0.51881 val_hamming_loss:0.00848 val_micro_f1score:0.56457\n",
            "epoch:9 loss:1.10150 hamming_loss:0.00947 micro_f1score:0.53256 val_hamming_loss:0.01169 val_micro_f1score:0.41839\n",
            "epoch:10 loss:1.05228 hamming_loss:0.00943 micro_f1score:0.55327 val_hamming_loss:0.00795 val_micro_f1score:0.61508\n",
            "epoch:11 loss:0.95847 hamming_loss:0.00861 micro_f1score:0.59393 val_hamming_loss:0.00768 val_micro_f1score:0.62860\n",
            "epoch:12 loss:0.91438 hamming_loss:0.00801 micro_f1score:0.62766 val_hamming_loss:0.00743 val_micro_f1score:0.64685\n",
            "epoch:13 loss:0.86809 hamming_loss:0.00776 micro_f1score:0.64148 val_hamming_loss:0.00710 val_micro_f1score:0.68046\n",
            "epoch:14 loss:0.83414 hamming_loss:0.00752 micro_f1score:0.65764 val_hamming_loss:0.00694 val_micro_f1score:0.68077\n",
            "epoch:15 loss:0.83137 hamming_loss:0.00742 micro_f1score:0.66294 val_hamming_loss:0.00706 val_micro_f1score:0.67202\n",
            "epoch:16 loss:0.79993 hamming_loss:0.00732 micro_f1score:0.67180 val_hamming_loss:0.00695 val_micro_f1score:0.67660\n",
            "epoch:17 loss:0.76688 hamming_loss:0.00703 micro_f1score:0.68953 val_hamming_loss:0.00696 val_micro_f1score:0.67604\n",
            "epoch:18 loss:0.75481 hamming_loss:0.00691 micro_f1score:0.69313 val_hamming_loss:0.00652 val_micro_f1score:0.70730\n",
            "epoch:19 loss:0.74475 hamming_loss:0.00695 micro_f1score:0.69503 val_hamming_loss:0.00671 val_micro_f1score:0.69147\n",
            "epoch:20 loss:0.71454 hamming_loss:0.00670 micro_f1score:0.70527 val_hamming_loss:0.00652 val_micro_f1score:0.71168\n",
            "epoch:21 loss:0.71403 hamming_loss:0.00677 micro_f1score:0.70593 val_hamming_loss:0.00643 val_micro_f1score:0.71297\n",
            "epoch:22 loss:0.68763 hamming_loss:0.00658 micro_f1score:0.71467 val_hamming_loss:0.00628 val_micro_f1score:0.72508\n",
            "epoch:23 loss:0.65449 hamming_loss:0.00627 micro_f1score:0.73186 val_hamming_loss:0.00639 val_micro_f1score:0.71417\n",
            "epoch:24 loss:0.63802 hamming_loss:0.00613 micro_f1score:0.73966 val_hamming_loss:0.00666 val_micro_f1score:0.69007\n",
            "epoch:25 loss:0.64922 hamming_loss:0.00626 micro_f1score:0.73339 val_hamming_loss:0.00662 val_micro_f1score:0.70338\n",
            "epoch:26 loss:0.64099 hamming_loss:0.00619 micro_f1score:0.73843 val_hamming_loss:0.00622 val_micro_f1score:0.72198\n",
            "epoch:27 loss:0.62746 hamming_loss:0.00602 micro_f1score:0.74622 val_hamming_loss:0.00608 val_micro_f1score:0.73144\n",
            "epoch:28 loss:0.61374 hamming_loss:0.00597 micro_f1score:0.75042 val_hamming_loss:0.00626 val_micro_f1score:0.71914\n",
            "epoch:29 loss:0.59760 hamming_loss:0.00582 micro_f1score:0.75604 val_hamming_loss:0.00635 val_micro_f1score:0.71738\n",
            "epoch:30 loss:0.60877 hamming_loss:0.00595 micro_f1score:0.75121 val_hamming_loss:0.00591 val_micro_f1score:0.74879\n",
            "epoch:31 loss:0.59898 hamming_loss:0.00574 micro_f1score:0.76024 val_hamming_loss:0.00585 val_micro_f1score:0.76069\n",
            "epoch:32 loss:0.57505 hamming_loss:0.00563 micro_f1score:0.76551 val_hamming_loss:0.00583 val_micro_f1score:0.75769\n",
            "epoch:33 loss:0.53388 hamming_loss:0.00541 micro_f1score:0.77670 val_hamming_loss:0.00559 val_micro_f1score:0.76541\n",
            "epoch:34 loss:0.51158 hamming_loss:0.00519 micro_f1score:0.78600 val_hamming_loss:0.00575 val_micro_f1score:0.76329\n",
            "epoch:35 loss:0.50122 hamming_loss:0.00517 micro_f1score:0.78853 val_hamming_loss:0.00548 val_micro_f1score:0.77513\n",
            "epoch:36 loss:0.48655 hamming_loss:0.00501 micro_f1score:0.79495 val_hamming_loss:0.00554 val_micro_f1score:0.77159\n",
            "epoch:37 loss:0.47297 hamming_loss:0.00491 micro_f1score:0.80040 val_hamming_loss:0.00544 val_micro_f1score:0.77071\n",
            "epoch:38 loss:0.46889 hamming_loss:0.00495 micro_f1score:0.79900 val_hamming_loss:0.00563 val_micro_f1score:0.77114\n",
            "epoch:39 loss:0.47246 hamming_loss:0.00486 micro_f1score:0.80399 val_hamming_loss:0.00544 val_micro_f1score:0.77667\n",
            "epoch:40 loss:0.45447 hamming_loss:0.00472 micro_f1score:0.80932 val_hamming_loss:0.00544 val_micro_f1score:0.77642\n",
            "epoch:41 loss:0.43985 hamming_loss:0.00467 micro_f1score:0.81267 val_hamming_loss:0.00558 val_micro_f1score:0.77246\n",
            "epoch:42 loss:0.44584 hamming_loss:0.00477 micro_f1score:0.80905 val_hamming_loss:0.00546 val_micro_f1score:0.77836\n",
            "epoch:43 loss:0.43449 hamming_loss:0.00460 micro_f1score:0.81544 val_hamming_loss:0.00573 val_micro_f1score:0.77348\n",
            "epoch:44 loss:0.42210 hamming_loss:0.00451 micro_f1score:0.82026 val_hamming_loss:0.00551 val_micro_f1score:0.77633\n",
            "epoch:45 loss:0.41776 hamming_loss:0.00458 micro_f1score:0.81739 val_hamming_loss:0.00538 val_micro_f1score:0.78193\n",
            "epoch:46 loss:0.42654 hamming_loss:0.00454 micro_f1score:0.81986 val_hamming_loss:0.00538 val_micro_f1score:0.77568\n",
            "epoch:47 loss:0.41570 hamming_loss:0.00453 micro_f1score:0.81888 val_hamming_loss:0.00533 val_micro_f1score:0.77880\n",
            "epoch:48 loss:0.40350 hamming_loss:0.00442 micro_f1score:0.82492 val_hamming_loss:0.00530 val_micro_f1score:0.78110\n",
            "epoch:49 loss:0.39690 hamming_loss:0.00428 micro_f1score:0.83028 val_hamming_loss:0.00566 val_micro_f1score:0.76800\n",
            "epoch:50 loss:0.39417 hamming_loss:0.00419 micro_f1score:0.83507 val_hamming_loss:0.00536 val_micro_f1score:0.77718\n",
            "epoch:51 loss:0.39753 hamming_loss:0.00439 micro_f1score:0.82707 val_hamming_loss:0.00519 val_micro_f1score:0.79178\n",
            "epoch:52 loss:0.37788 hamming_loss:0.00409 micro_f1score:0.83889 val_hamming_loss:0.00515 val_micro_f1score:0.79084\n",
            "epoch:53 loss:0.36947 hamming_loss:0.00406 micro_f1score:0.84119 val_hamming_loss:0.00523 val_micro_f1score:0.78551\n",
            "epoch:54 loss:0.35925 hamming_loss:0.00403 micro_f1score:0.84267 val_hamming_loss:0.00524 val_micro_f1score:0.78003\n",
            "epoch:55 loss:0.36661 hamming_loss:0.00406 micro_f1score:0.84184 val_hamming_loss:0.00505 val_micro_f1score:0.79175\n",
            "epoch:56 loss:0.35308 hamming_loss:0.00385 micro_f1score:0.84984 val_hamming_loss:0.00521 val_micro_f1score:0.78674\n",
            "epoch:57 loss:0.38617 hamming_loss:0.00413 micro_f1score:0.83848 val_hamming_loss:0.00515 val_micro_f1score:0.79308\n",
            "epoch:58 loss:0.35514 hamming_loss:0.00386 micro_f1score:0.85019 val_hamming_loss:0.00519 val_micro_f1score:0.79595\n",
            "epoch:59 loss:0.34858 hamming_loss:0.00393 micro_f1score:0.84710 val_hamming_loss:0.00553 val_micro_f1score:0.79419\n",
            "epoch:60 loss:0.37633 hamming_loss:0.00399 micro_f1score:0.84466 val_hamming_loss:0.00520 val_micro_f1score:0.79742\n",
            "epoch:61 loss:0.37526 hamming_loss:0.00401 micro_f1score:0.84477 val_hamming_loss:0.00554 val_micro_f1score:0.77406\n",
            "epoch:62 loss:0.37908 hamming_loss:0.00403 micro_f1score:0.84192 val_hamming_loss:0.00498 val_micro_f1score:0.80188\n",
            "epoch:63 loss:0.37704 hamming_loss:0.00399 micro_f1score:0.84420 val_hamming_loss:0.00504 val_micro_f1score:0.80299\n",
            "epoch:64 loss:0.33932 hamming_loss:0.00372 micro_f1score:0.85631 val_hamming_loss:0.00488 val_micro_f1score:0.80226\n",
            "epoch:65 loss:0.31406 hamming_loss:0.00333 micro_f1score:0.87128 val_hamming_loss:0.00500 val_micro_f1score:0.80381\n",
            "epoch:66 loss:0.40273 hamming_loss:0.00431 micro_f1score:0.83209 val_hamming_loss:0.00508 val_micro_f1score:0.79688\n",
            "epoch:67 loss:0.34208 hamming_loss:0.00371 micro_f1score:0.85675 val_hamming_loss:0.00487 val_micro_f1score:0.80186\n",
            "epoch:68 loss:0.31380 hamming_loss:0.00349 micro_f1score:0.86577 val_hamming_loss:0.00478 val_micro_f1score:0.80928\n",
            "epoch:69 loss:0.29962 hamming_loss:0.00327 micro_f1score:0.87444 val_hamming_loss:0.00495 val_micro_f1score:0.79568\n",
            "epoch:70 loss:0.29738 hamming_loss:0.00331 micro_f1score:0.87325 val_hamming_loss:0.00494 val_micro_f1score:0.80409\n",
            "epoch:71 loss:0.29275 hamming_loss:0.00326 micro_f1score:0.87537 val_hamming_loss:0.00483 val_micro_f1score:0.80418\n",
            "epoch:72 loss:0.28593 hamming_loss:0.00318 micro_f1score:0.87842 val_hamming_loss:0.00481 val_micro_f1score:0.80449\n",
            "epoch:73 loss:0.27483 hamming_loss:0.00308 micro_f1score:0.88278 val_hamming_loss:0.00467 val_micro_f1score:0.80999\n",
            "epoch:74 loss:0.26727 hamming_loss:0.00308 micro_f1score:0.88257 val_hamming_loss:0.00469 val_micro_f1score:0.81197\n",
            "epoch:75 loss:0.27040 hamming_loss:0.00305 micro_f1score:0.88339 val_hamming_loss:0.00479 val_micro_f1score:0.80285\n",
            "epoch:76 loss:0.27848 hamming_loss:0.00308 micro_f1score:0.88201 val_hamming_loss:0.00478 val_micro_f1score:0.80422\n",
            "epoch:77 loss:0.28326 hamming_loss:0.00313 micro_f1score:0.88055 val_hamming_loss:0.00496 val_micro_f1score:0.79401\n",
            "epoch:78 loss:0.26384 hamming_loss:0.00294 micro_f1score:0.88804 val_hamming_loss:0.00472 val_micro_f1score:0.80750\n",
            "epoch:79 loss:0.26181 hamming_loss:0.00299 micro_f1score:0.88630 val_hamming_loss:0.00459 val_micro_f1score:0.81285\n",
            "epoch:80 loss:0.25562 hamming_loss:0.00287 micro_f1score:0.89105 val_hamming_loss:0.00500 val_micro_f1score:0.79293\n",
            "epoch:81 loss:0.25275 hamming_loss:0.00282 micro_f1score:0.89327 val_hamming_loss:0.00474 val_micro_f1score:0.80366\n",
            "epoch:82 loss:0.24756 hamming_loss:0.00277 micro_f1score:0.89487 val_hamming_loss:0.00470 val_micro_f1score:0.80648\n",
            "epoch:83 loss:0.24906 hamming_loss:0.00286 micro_f1score:0.89156 val_hamming_loss:0.00466 val_micro_f1score:0.81083\n",
            "epoch:84 loss:0.24998 hamming_loss:0.00281 micro_f1score:0.89316 val_hamming_loss:0.00483 val_micro_f1score:0.80638\n",
            "epoch:85 loss:0.26195 hamming_loss:0.00297 micro_f1score:0.88718 val_hamming_loss:0.00454 val_micro_f1score:0.81828\n",
            "epoch:86 loss:0.24735 hamming_loss:0.00277 micro_f1score:0.89467 val_hamming_loss:0.00461 val_micro_f1score:0.81652\n",
            "epoch:87 loss:0.24593 hamming_loss:0.00272 micro_f1score:0.89675 val_hamming_loss:0.00476 val_micro_f1score:0.81645\n",
            "epoch:88 loss:0.24636 hamming_loss:0.00271 micro_f1score:0.89761 val_hamming_loss:0.00467 val_micro_f1score:0.81654\n",
            "epoch:89 loss:0.24733 hamming_loss:0.00279 micro_f1score:0.89431 val_hamming_loss:0.00460 val_micro_f1score:0.81499\n",
            "epoch:90 loss:0.23223 hamming_loss:0.00253 micro_f1score:0.90406 val_hamming_loss:0.00467 val_micro_f1score:0.81754\n",
            "epoch:91 loss:0.23314 hamming_loss:0.00262 micro_f1score:0.90131 val_hamming_loss:0.00441 val_micro_f1score:0.82473\n",
            "epoch:92 loss:0.23405 hamming_loss:0.00268 micro_f1score:0.89850 val_hamming_loss:0.00450 val_micro_f1score:0.81926\n",
            "epoch:93 loss:0.22339 hamming_loss:0.00251 micro_f1score:0.90518 val_hamming_loss:0.00440 val_micro_f1score:0.82697\n",
            "epoch:94 loss:0.21833 hamming_loss:0.00253 micro_f1score:0.90449 val_hamming_loss:0.00462 val_micro_f1score:0.82070\n",
            "epoch:95 loss:0.22213 hamming_loss:0.00248 micro_f1score:0.90660 val_hamming_loss:0.00450 val_micro_f1score:0.82143\n",
            "epoch:96 loss:0.21366 hamming_loss:0.00243 micro_f1score:0.90846 val_hamming_loss:0.00463 val_micro_f1score:0.81695\n",
            "epoch:97 loss:0.21324 hamming_loss:0.00243 micro_f1score:0.90849 val_hamming_loss:0.00446 val_micro_f1score:0.82455\n",
            "epoch:98 loss:0.21226 hamming_loss:0.00248 micro_f1score:0.90655 val_hamming_loss:0.00435 val_micro_f1score:0.82980\n",
            "epoch:99 loss:0.21213 hamming_loss:0.00246 micro_f1score:0.90778 val_hamming_loss:0.00450 val_micro_f1score:0.82317\n",
            "epoch:100 loss:0.20958 hamming_loss:0.00238 micro_f1score:0.91044 val_hamming_loss:0.00443 val_micro_f1score:0.82614\n",
            "epoch:101 loss:0.21029 hamming_loss:0.00239 micro_f1score:0.91049 val_hamming_loss:0.00443 val_micro_f1score:0.82406\n",
            "epoch:102 loss:0.21318 hamming_loss:0.00242 micro_f1score:0.90910 val_hamming_loss:0.00452 val_micro_f1score:0.81681\n",
            "epoch:103 loss:0.21235 hamming_loss:0.00239 micro_f1score:0.91034 val_hamming_loss:0.00439 val_micro_f1score:0.82612\n",
            "epoch:104 loss:0.20531 hamming_loss:0.00234 micro_f1score:0.91253 val_hamming_loss:0.00445 val_micro_f1score:0.82218\n",
            "epoch:105 loss:0.20025 hamming_loss:0.00227 micro_f1score:0.91479 val_hamming_loss:0.00428 val_micro_f1score:0.82967\n",
            "epoch:106 loss:0.19948 hamming_loss:0.00232 micro_f1score:0.91310 val_hamming_loss:0.00440 val_micro_f1score:0.82443\n",
            "epoch:107 loss:0.19548 hamming_loss:0.00221 micro_f1score:0.91698 val_hamming_loss:0.00437 val_micro_f1score:0.82837\n",
            "epoch:108 loss:0.19692 hamming_loss:0.00223 micro_f1score:0.91647 val_hamming_loss:0.00450 val_micro_f1score:0.82640\n",
            "epoch:109 loss:0.20145 hamming_loss:0.00225 micro_f1score:0.91540 val_hamming_loss:0.00445 val_micro_f1score:0.82751\n",
            "epoch:110 loss:0.21115 hamming_loss:0.00239 micro_f1score:0.91050 val_hamming_loss:0.00443 val_micro_f1score:0.82603\n",
            "epoch:111 loss:0.18556 hamming_loss:0.00214 micro_f1score:0.91995 val_hamming_loss:0.00429 val_micro_f1score:0.83067\n",
            "epoch:112 loss:0.18998 hamming_loss:0.00216 micro_f1score:0.91924 val_hamming_loss:0.00427 val_micro_f1score:0.82919\n",
            "epoch:113 loss:0.18216 hamming_loss:0.00211 micro_f1score:0.92115 val_hamming_loss:0.00428 val_micro_f1score:0.83213\n",
            "epoch:114 loss:0.17787 hamming_loss:0.00200 micro_f1score:0.92523 val_hamming_loss:0.00424 val_micro_f1score:0.83275\n",
            "epoch:115 loss:0.17000 hamming_loss:0.00196 micro_f1score:0.92699 val_hamming_loss:0.00435 val_micro_f1score:0.83063\n",
            "epoch:116 loss:0.17164 hamming_loss:0.00196 micro_f1score:0.92678 val_hamming_loss:0.00423 val_micro_f1score:0.83413\n",
            "epoch:117 loss:0.16984 hamming_loss:0.00193 micro_f1score:0.92782 val_hamming_loss:0.00436 val_micro_f1score:0.82960\n",
            "epoch:118 loss:0.17121 hamming_loss:0.00200 micro_f1score:0.92564 val_hamming_loss:0.00428 val_micro_f1score:0.82977\n",
            "epoch:119 loss:0.18009 hamming_loss:0.00207 micro_f1score:0.92292 val_hamming_loss:0.00434 val_micro_f1score:0.82948\n",
            "epoch:120 loss:0.18060 hamming_loss:0.00211 micro_f1score:0.92136 val_hamming_loss:0.00433 val_micro_f1score:0.83011\n",
            "epoch:121 loss:0.18197 hamming_loss:0.00201 micro_f1score:0.92519 val_hamming_loss:0.00451 val_micro_f1score:0.82471\n",
            "epoch:122 loss:0.22380 hamming_loss:0.00243 micro_f1score:0.90935 val_hamming_loss:0.00470 val_micro_f1score:0.81239\n",
            "epoch:123 loss:0.37584 hamming_loss:0.00384 micro_f1score:0.85543 val_hamming_loss:0.00553 val_micro_f1score:0.78246\n",
            "epoch:124 loss:0.42427 hamming_loss:0.00446 micro_f1score:0.83127 val_hamming_loss:0.00539 val_micro_f1score:0.78092\n",
            "epoch:125 loss:0.34191 hamming_loss:0.00378 micro_f1score:0.85677 val_hamming_loss:0.00467 val_micro_f1score:0.81247\n",
            "epoch:126 loss:0.26050 hamming_loss:0.00300 micro_f1score:0.88690 val_hamming_loss:0.00470 val_micro_f1score:0.80754\n",
            "epoch:127 loss:0.25498 hamming_loss:0.00288 micro_f1score:0.89186 val_hamming_loss:0.00452 val_micro_f1score:0.81989\n",
            "epoch:128 loss:0.22414 hamming_loss:0.00252 micro_f1score:0.90549 val_hamming_loss:0.00448 val_micro_f1score:0.82407\n",
            "epoch:129 loss:0.20493 hamming_loss:0.00236 micro_f1score:0.91176 val_hamming_loss:0.00452 val_micro_f1score:0.82569\n",
            "epoch:130 loss:0.20453 hamming_loss:0.00242 micro_f1score:0.90948 val_hamming_loss:0.00430 val_micro_f1score:0.83119\n",
            "epoch:131 loss:0.19740 hamming_loss:0.00228 micro_f1score:0.91480 val_hamming_loss:0.00436 val_micro_f1score:0.83141\n",
            "epoch:132 loss:0.19870 hamming_loss:0.00222 micro_f1score:0.91669 val_hamming_loss:0.00455 val_micro_f1score:0.82593\n",
            "epoch:133 loss:0.19545 hamming_loss:0.00228 micro_f1score:0.91498 val_hamming_loss:0.00427 val_micro_f1score:0.83018\n",
            "epoch:134 loss:0.17292 hamming_loss:0.00202 micro_f1score:0.92485 val_hamming_loss:0.00444 val_micro_f1score:0.83002\n",
            "epoch:135 loss:0.18362 hamming_loss:0.00210 micro_f1score:0.92204 val_hamming_loss:0.00435 val_micro_f1score:0.83184\n",
            "epoch:136 loss:0.17474 hamming_loss:0.00204 micro_f1score:0.92421 val_hamming_loss:0.00433 val_micro_f1score:0.82981\n",
            "epoch:137 loss:0.17379 hamming_loss:0.00200 micro_f1score:0.92548 val_hamming_loss:0.00434 val_micro_f1score:0.82989\n",
            "epoch:138 loss:0.17009 hamming_loss:0.00201 micro_f1score:0.92532 val_hamming_loss:0.00428 val_micro_f1score:0.83364\n",
            "epoch:139 loss:0.16132 hamming_loss:0.00194 micro_f1score:0.92818 val_hamming_loss:0.00422 val_micro_f1score:0.83312\n",
            "epoch:140 loss:0.15559 hamming_loss:0.00179 micro_f1score:0.93342 val_hamming_loss:0.00414 val_micro_f1score:0.83836\n",
            "epoch:141 loss:0.16485 hamming_loss:0.00190 micro_f1score:0.92946 val_hamming_loss:0.00439 val_micro_f1score:0.83176\n",
            "epoch:142 loss:0.16546 hamming_loss:0.00195 micro_f1score:0.92750 val_hamming_loss:0.00410 val_micro_f1score:0.84117\n",
            "epoch:143 loss:0.15777 hamming_loss:0.00188 micro_f1score:0.93024 val_hamming_loss:0.00420 val_micro_f1score:0.83700\n",
            "epoch:144 loss:0.15412 hamming_loss:0.00179 micro_f1score:0.93350 val_hamming_loss:0.00412 val_micro_f1score:0.83782\n",
            "epoch:145 loss:0.15692 hamming_loss:0.00185 micro_f1score:0.93124 val_hamming_loss:0.00418 val_micro_f1score:0.83673\n",
            "epoch:146 loss:0.15213 hamming_loss:0.00174 micro_f1score:0.93554 val_hamming_loss:0.00421 val_micro_f1score:0.83732\n",
            "epoch:147 loss:0.14901 hamming_loss:0.00171 micro_f1score:0.93638 val_hamming_loss:0.00430 val_micro_f1score:0.83459\n",
            "epoch:148 loss:0.15488 hamming_loss:0.00176 micro_f1score:0.93467 val_hamming_loss:0.00425 val_micro_f1score:0.83263\n",
            "epoch:149 loss:0.15187 hamming_loss:0.00179 micro_f1score:0.93342 val_hamming_loss:0.00416 val_micro_f1score:0.83806\n",
            "epoch:150 loss:0.15378 hamming_loss:0.00174 micro_f1score:0.93557 val_hamming_loss:0.00415 val_micro_f1score:0.83888\n",
            "epoch:151 loss:0.14700 hamming_loss:0.00177 micro_f1score:0.93448 val_hamming_loss:0.00419 val_micro_f1score:0.83469\n",
            "epoch:152 loss:0.15202 hamming_loss:0.00179 micro_f1score:0.93370 val_hamming_loss:0.00427 val_micro_f1score:0.83497\n",
            "epoch:153 loss:0.15403 hamming_loss:0.00179 micro_f1score:0.93368 val_hamming_loss:0.00430 val_micro_f1score:0.83310\n",
            "epoch:154 loss:0.14695 hamming_loss:0.00176 micro_f1score:0.93469 val_hamming_loss:0.00424 val_micro_f1score:0.83408\n",
            "epoch:155 loss:0.15228 hamming_loss:0.00169 micro_f1score:0.93720 val_hamming_loss:0.00424 val_micro_f1score:0.83731\n",
            "epoch:156 loss:0.18356 hamming_loss:0.00205 micro_f1score:0.92459 val_hamming_loss:0.00608 val_micro_f1score:0.73872\n",
            "epoch:157 loss:0.35677 hamming_loss:0.00358 micro_f1score:0.86555 val_hamming_loss:0.00438 val_micro_f1score:0.82724\n",
            "epoch:158 loss:0.23543 hamming_loss:0.00253 micro_f1score:0.90539 val_hamming_loss:0.00428 val_micro_f1score:0.83539\n",
            "epoch:159 loss:0.20531 hamming_loss:0.00228 micro_f1score:0.91504 val_hamming_loss:0.00408 val_micro_f1score:0.84316\n",
            "epoch:160 loss:0.17914 hamming_loss:0.00203 micro_f1score:0.92485 val_hamming_loss:0.00409 val_micro_f1score:0.84069\n",
            "epoch:161 loss:0.16778 hamming_loss:0.00194 micro_f1score:0.92816 val_hamming_loss:0.00416 val_micro_f1score:0.83923\n",
            "epoch:162 loss:0.16213 hamming_loss:0.00182 micro_f1score:0.93233 val_hamming_loss:0.00407 val_micro_f1score:0.84082\n",
            "epoch:163 loss:0.15916 hamming_loss:0.00183 micro_f1score:0.93205 val_hamming_loss:0.00402 val_micro_f1score:0.84310\n",
            "epoch:164 loss:0.14638 hamming_loss:0.00166 micro_f1score:0.93837 val_hamming_loss:0.00402 val_micro_f1score:0.84069\n",
            "epoch:165 loss:0.14955 hamming_loss:0.00170 micro_f1score:0.93720 val_hamming_loss:0.00408 val_micro_f1score:0.83749\n",
            "epoch:166 loss:0.15768 hamming_loss:0.00174 micro_f1score:0.93526 val_hamming_loss:0.00420 val_micro_f1score:0.83559\n",
            "epoch:167 loss:0.15361 hamming_loss:0.00181 micro_f1score:0.93281 val_hamming_loss:0.00406 val_micro_f1score:0.84026\n",
            "epoch:168 loss:0.14055 hamming_loss:0.00161 micro_f1score:0.94022 val_hamming_loss:0.00406 val_micro_f1score:0.83961\n",
            "epoch:169 loss:0.14214 hamming_loss:0.00161 micro_f1score:0.94037 val_hamming_loss:0.00406 val_micro_f1score:0.84045\n",
            "epoch:170 loss:0.13537 hamming_loss:0.00157 micro_f1score:0.94196 val_hamming_loss:0.00402 val_micro_f1score:0.84215\n",
            "epoch:171 loss:0.13134 hamming_loss:0.00149 micro_f1score:0.94481 val_hamming_loss:0.00398 val_micro_f1score:0.84328\n",
            "epoch:172 loss:0.12432 hamming_loss:0.00145 micro_f1score:0.94655 val_hamming_loss:0.00404 val_micro_f1score:0.84322\n",
            "epoch:173 loss:0.24467 hamming_loss:0.00249 micro_f1score:0.90740 val_hamming_loss:0.00478 val_micro_f1score:0.81759\n",
            "epoch:174 loss:0.50148 hamming_loss:0.00425 micro_f1score:0.84160 val_hamming_loss:0.00709 val_micro_f1score:0.74866\n",
            "epoch:175 loss:0.35884 hamming_loss:0.00377 micro_f1score:0.85940 val_hamming_loss:0.00426 val_micro_f1score:0.83110\n",
            "epoch:176 loss:0.23383 hamming_loss:0.00252 micro_f1score:0.90598 val_hamming_loss:0.00407 val_micro_f1score:0.84091\n",
            "epoch:177 loss:0.19307 hamming_loss:0.00210 micro_f1score:0.92166 val_hamming_loss:0.00411 val_micro_f1score:0.83872\n",
            "epoch:178 loss:0.17158 hamming_loss:0.00192 micro_f1score:0.92856 val_hamming_loss:0.00402 val_micro_f1score:0.84130\n",
            "epoch:179 loss:0.16191 hamming_loss:0.00185 micro_f1score:0.93143 val_hamming_loss:0.00407 val_micro_f1score:0.83941\n",
            "epoch:180 loss:0.15675 hamming_loss:0.00177 micro_f1score:0.93435 val_hamming_loss:0.00408 val_micro_f1score:0.84012\n",
            "epoch:181 loss:0.15591 hamming_loss:0.00176 micro_f1score:0.93479 val_hamming_loss:0.00409 val_micro_f1score:0.83893\n",
            "epoch:182 loss:0.14685 hamming_loss:0.00164 micro_f1score:0.93925 val_hamming_loss:0.00398 val_micro_f1score:0.84373\n",
            "epoch:183 loss:0.13697 hamming_loss:0.00162 micro_f1score:0.94009 val_hamming_loss:0.00406 val_micro_f1score:0.83958\n",
            "epoch:184 loss:0.14686 hamming_loss:0.00161 micro_f1score:0.94026 val_hamming_loss:0.00404 val_micro_f1score:0.84127\n",
            "epoch:185 loss:0.13590 hamming_loss:0.00154 micro_f1score:0.94301 val_hamming_loss:0.00404 val_micro_f1score:0.84115\n",
            "epoch:186 loss:0.12919 hamming_loss:0.00152 micro_f1score:0.94365 val_hamming_loss:0.00396 val_micro_f1score:0.84566\n",
            "epoch:187 loss:0.12901 hamming_loss:0.00148 micro_f1score:0.94521 val_hamming_loss:0.00404 val_micro_f1score:0.84105\n",
            "epoch:188 loss:0.12803 hamming_loss:0.00152 micro_f1score:0.94373 val_hamming_loss:0.00404 val_micro_f1score:0.84208\n",
            "epoch:189 loss:0.13111 hamming_loss:0.00150 micro_f1score:0.94473 val_hamming_loss:0.00405 val_micro_f1score:0.84081\n",
            "epoch:190 loss:0.12597 hamming_loss:0.00146 micro_f1score:0.94596 val_hamming_loss:0.00393 val_micro_f1score:0.84738\n",
            "epoch:191 loss:0.12220 hamming_loss:0.00142 micro_f1score:0.94738 val_hamming_loss:0.00397 val_micro_f1score:0.84393\n",
            "epoch:192 loss:0.12778 hamming_loss:0.00142 micro_f1score:0.94762 val_hamming_loss:0.00400 val_micro_f1score:0.84200\n",
            "epoch:193 loss:0.12441 hamming_loss:0.00139 micro_f1score:0.94863 val_hamming_loss:0.00400 val_micro_f1score:0.84290\n",
            "epoch:194 loss:0.11968 hamming_loss:0.00138 micro_f1score:0.94902 val_hamming_loss:0.00400 val_micro_f1score:0.84226\n",
            "epoch:195 loss:0.11630 hamming_loss:0.00131 micro_f1score:0.95162 val_hamming_loss:0.00402 val_micro_f1score:0.84297\n",
            "epoch:196 loss:0.12020 hamming_loss:0.00138 micro_f1score:0.94907 val_hamming_loss:0.00400 val_micro_f1score:0.84247\n",
            "epoch:197 loss:0.11905 hamming_loss:0.00135 micro_f1score:0.95021 val_hamming_loss:0.00401 val_micro_f1score:0.84289\n",
            "epoch:198 loss:0.11959 hamming_loss:0.00139 micro_f1score:0.94883 val_hamming_loss:0.00403 val_micro_f1score:0.84068\n",
            "epoch:199 loss:0.12197 hamming_loss:0.00139 micro_f1score:0.94873 val_hamming_loss:0.00414 val_micro_f1score:0.83490\n",
            "epoch:200 loss:0.12818 hamming_loss:0.00147 micro_f1score:0.94579 val_hamming_loss:0.00394 val_micro_f1score:0.84493\n",
            "epoch:201 loss:0.12449 hamming_loss:0.00147 micro_f1score:0.94550 val_hamming_loss:0.00391 val_micro_f1score:0.84543\n",
            "epoch:202 loss:0.11078 hamming_loss:0.00137 micro_f1score:0.94938 val_hamming_loss:0.00397 val_micro_f1score:0.84465\n",
            "epoch:203 loss:0.11204 hamming_loss:0.00130 micro_f1score:0.95199 val_hamming_loss:0.00393 val_micro_f1score:0.84673\n",
            "epoch:204 loss:0.11148 hamming_loss:0.00131 micro_f1score:0.95156 val_hamming_loss:0.00400 val_micro_f1score:0.84177\n",
            "epoch:205 loss:0.11299 hamming_loss:0.00128 micro_f1score:0.95282 val_hamming_loss:0.00397 val_micro_f1score:0.84411\n",
            "epoch:206 loss:0.11901 hamming_loss:0.00143 micro_f1score:0.94726 val_hamming_loss:0.00404 val_micro_f1score:0.84141\n",
            "epoch:207 loss:0.13158 hamming_loss:0.00145 micro_f1score:0.94644 val_hamming_loss:0.00382 val_micro_f1score:0.85040\n",
            "epoch:208 loss:0.11889 hamming_loss:0.00136 micro_f1score:0.94965 val_hamming_loss:0.00383 val_micro_f1score:0.85041\n",
            "epoch:209 loss:0.11454 hamming_loss:0.00126 micro_f1score:0.95348 val_hamming_loss:0.00389 val_micro_f1score:0.84715\n",
            "epoch:210 loss:0.11349 hamming_loss:0.00129 micro_f1score:0.95242 val_hamming_loss:0.00394 val_micro_f1score:0.84641\n",
            "epoch:211 loss:0.11311 hamming_loss:0.00133 micro_f1score:0.95106 val_hamming_loss:0.00401 val_micro_f1score:0.84252\n",
            "epoch:212 loss:0.11571 hamming_loss:0.00131 micro_f1score:0.95173 val_hamming_loss:0.00402 val_micro_f1score:0.84149\n",
            "epoch:213 loss:0.11802 hamming_loss:0.00134 micro_f1score:0.95075 val_hamming_loss:0.00402 val_micro_f1score:0.84027\n",
            "epoch:214 loss:0.11992 hamming_loss:0.00132 micro_f1score:0.95140 val_hamming_loss:0.00393 val_micro_f1score:0.84522\n",
            "epoch:215 loss:0.11695 hamming_loss:0.00128 micro_f1score:0.95274 val_hamming_loss:0.00410 val_micro_f1score:0.83839\n",
            "epoch:216 loss:0.12212 hamming_loss:0.00141 micro_f1score:0.94789 val_hamming_loss:0.00406 val_micro_f1score:0.83942\n",
            "epoch:217 loss:0.11870 hamming_loss:0.00138 micro_f1score:0.94897 val_hamming_loss:0.00391 val_micro_f1score:0.84681\n",
            "epoch:218 loss:0.10628 hamming_loss:0.00125 micro_f1score:0.95394 val_hamming_loss:0.00393 val_micro_f1score:0.84707\n",
            "epoch:219 loss:0.11456 hamming_loss:0.00132 micro_f1score:0.95126 val_hamming_loss:0.00397 val_micro_f1score:0.84667\n",
            "epoch:220 loss:0.11685 hamming_loss:0.00138 micro_f1score:0.94913 val_hamming_loss:0.00392 val_micro_f1score:0.84845\n",
            "epoch:221 loss:0.11494 hamming_loss:0.00136 micro_f1score:0.95006 val_hamming_loss:0.00400 val_micro_f1score:0.84264\n",
            "epoch:222 loss:0.11039 hamming_loss:0.00130 micro_f1score:0.95219 val_hamming_loss:0.00393 val_micro_f1score:0.84599\n",
            "epoch:223 loss:0.11033 hamming_loss:0.00128 micro_f1score:0.95295 val_hamming_loss:0.00397 val_micro_f1score:0.84490\n",
            "epoch:224 loss:0.11122 hamming_loss:0.00123 micro_f1score:0.95470 val_hamming_loss:0.00403 val_micro_f1score:0.83995\n",
            "epoch:225 loss:0.11929 hamming_loss:0.00135 micro_f1score:0.95012 val_hamming_loss:0.00403 val_micro_f1score:0.84172\n",
            "epoch:226 loss:0.11236 hamming_loss:0.00122 micro_f1score:0.95486 val_hamming_loss:0.00420 val_micro_f1score:0.83464\n",
            "epoch:227 loss:0.11646 hamming_loss:0.00131 micro_f1score:0.95152 val_hamming_loss:0.00397 val_micro_f1score:0.84368\n",
            "epoch:228 loss:0.10576 hamming_loss:0.00120 micro_f1score:0.95586 val_hamming_loss:0.00409 val_micro_f1score:0.84047\n",
            "epoch:229 loss:0.11018 hamming_loss:0.00126 micro_f1score:0.95359 val_hamming_loss:0.00396 val_micro_f1score:0.84546\n",
            "epoch:230 loss:0.11897 hamming_loss:0.00135 micro_f1score:0.95013 val_hamming_loss:0.00403 val_micro_f1score:0.84179\n",
            "epoch:231 loss:0.10859 hamming_loss:0.00127 micro_f1score:0.95331 val_hamming_loss:0.00400 val_micro_f1score:0.84389\n",
            "epoch:232 loss:0.11096 hamming_loss:0.00128 micro_f1score:0.95289 val_hamming_loss:0.00401 val_micro_f1score:0.84183\n",
            "epoch:233 loss:0.11130 hamming_loss:0.00132 micro_f1score:0.95157 val_hamming_loss:0.00402 val_micro_f1score:0.84149\n",
            "epoch:234 loss:0.12706 hamming_loss:0.00144 micro_f1score:0.94675 val_hamming_loss:0.00403 val_micro_f1score:0.83854\n",
            "epoch:235 loss:0.10951 hamming_loss:0.00127 micro_f1score:0.95328 val_hamming_loss:0.00396 val_micro_f1score:0.84323\n",
            "epoch:236 loss:0.10979 hamming_loss:0.00125 micro_f1score:0.95386 val_hamming_loss:0.00402 val_micro_f1score:0.84247\n",
            "epoch:237 loss:0.11025 hamming_loss:0.00127 micro_f1score:0.95309 val_hamming_loss:0.00395 val_micro_f1score:0.84607\n",
            "epoch:238 loss:0.11629 hamming_loss:0.00129 micro_f1score:0.95229 val_hamming_loss:0.00410 val_micro_f1score:0.83860\n",
            "epoch:239 loss:0.12675 hamming_loss:0.00142 micro_f1score:0.94752 val_hamming_loss:0.00395 val_micro_f1score:0.84594\n",
            "epoch:240 loss:0.11766 hamming_loss:0.00132 micro_f1score:0.95129 val_hamming_loss:0.00388 val_micro_f1score:0.84765\n",
            "epoch:241 loss:0.10540 hamming_loss:0.00122 micro_f1score:0.95507 val_hamming_loss:0.00393 val_micro_f1score:0.84643\n",
            "epoch:242 loss:0.10299 hamming_loss:0.00123 micro_f1score:0.95485 val_hamming_loss:0.00390 val_micro_f1score:0.84625\n",
            "epoch:243 loss:0.09920 hamming_loss:0.00119 micro_f1score:0.95630 val_hamming_loss:0.00390 val_micro_f1score:0.84705\n",
            "epoch:244 loss:0.12316 hamming_loss:0.00141 micro_f1score:0.94808 val_hamming_loss:0.00418 val_micro_f1score:0.83704\n",
            "epoch:245 loss:0.13223 hamming_loss:0.00147 micro_f1score:0.94577 val_hamming_loss:0.00417 val_micro_f1score:0.83589\n",
            "epoch:246 loss:0.14155 hamming_loss:0.00153 micro_f1score:0.94354 val_hamming_loss:0.00421 val_micro_f1score:0.83152\n",
            "epoch:247 loss:0.13048 hamming_loss:0.00154 micro_f1score:0.94321 val_hamming_loss:0.00400 val_micro_f1score:0.84419\n",
            "epoch:248 loss:0.14240 hamming_loss:0.00153 micro_f1score:0.94347 val_hamming_loss:0.00436 val_micro_f1score:0.82637\n",
            "epoch:249 loss:0.24656 hamming_loss:0.00241 micro_f1score:0.91064 val_hamming_loss:0.00431 val_micro_f1score:0.82467\n",
            "epoch:250 loss:0.24209 hamming_loss:0.00255 micro_f1score:0.90542 val_hamming_loss:0.00417 val_micro_f1score:0.83733\n"
          ]
        }
      ]
    }
  ]
}